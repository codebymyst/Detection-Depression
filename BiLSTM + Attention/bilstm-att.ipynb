{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":7343021,"sourceType":"datasetVersion","datasetId":4263651}],"dockerImageVersionId":30827,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade torch\n!pip install --upgrade torchaudio\n!pip install --upgrade torchdata\n!pip install --upgrade torchtext\n!pip install --upgrade torchvision\n!pip install --upgrade torch-xla\n!pip install --upgrade transformers\n!pip install --upgrade scikeras\n# !pip install --upgrade tensorflow\n!pip install tensorflow==2.15.1\n!pip install keras-core\n!pip install --upgrade scikeras","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install nltk\n!pip install --upgrade nltk\n!pip show nltk\n\nimport nltk\n\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('punkt_tab')\nnltk.download('averaged_perceptron_tagger_eng')\nnltk.download('words')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Shen Dataset\nimport glob\nimport json\nimport pandas as pd\n\n\ndirectory_path = '/kaggle/input/mddl-shennn/Dataset/labeled/negative/data/tweet/'\n\nfile_paths = glob.glob(directory_path + '*.json')\n\n\ntexts = []\nlabels = []\n\n\nfor file_path in file_paths:\n    with open(file_path, 'r') as file:\n        json_data = json.load(file)\n        text = json_data['text'] \n        texts.append(text)\n        labels.append(0)  \n\n\ndata_healthy = pd.DataFrame({'text': texts, 'label': labels})\n\n\n\ndirectory_path = '/kaggle/input/mddl-shennn/Dataset/labeled/positive/data/tweet/'\n\n\nfile_paths = glob.glob(directory_path + '*.json')\n\ntexts = []\nlabels = []\n\n\nfor file_path in file_paths:\n    with open(file_path, 'r') as file:\n        json_data = json.load(file)\n        text = json_data['text']  \n        texts.append(text)\n        labels.append(1) \n\n\ndata_depression = pd.DataFrame({'text': texts, 'label': labels})\n\n\ndata_shen = pd.concat([data_healthy, data_depression], axis=0, ignore_index=True)\n\n\ndata_shen = data_shen [data_shen ['text'] != \"\"]\ndata_shen_duplicates = data_shen.drop_duplicates()\n\ndata_shen_duplicates = data_shen_duplicates.reset_index(drop=True)\n\n\nx_shen = data_shen_duplicates.text\ny_shen = data_shen_duplicates.label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\nlabel_counts = Counter(y_shen)\nprint(label_counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport unicodedata\nfrom nltk.tokenize import RegexpTokenizer\n\nimport nltk\nfrom nltk.corpus import words\nfrom nltk.tokenize import word_tokenize\n\n\ndef remove_usernames(text):\n    username_pattern = re.compile(r'@[\\w]+')\n    text_without_usernames = username_pattern.sub('', text)\n    return text_without_usernames\n\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    text_without_urls = url_pattern.sub('', text)\n    return text_without_urls\n\n\ndef remove_hashtags(text):\n    hashtag_pattern = re.compile(r'#\\w+')\n    text_without_hashtags = hashtag_pattern.sub('', text)\n    return text_without_hashtags\n\n\ndef remove_emojis(text):\n    emoji_pattern = re.compile(\n        \"[\"\n        \"\\U0001F600-\\U0001F64F\"  \n        \"\\U0001F300-\\U0001F5FF\"  \n        \"\\U0001F680-\\U0001F6FF\"  \n        \"\\U0001F700-\\U0001F77F\"  \n        \"\\U0001F780-\\U0001F7FF\"  \n        \"\\U0001F800-\\U0001F8FF\"  \n        \"\\U0001F900-\\U0001F9FF\"  \n        \"\\U0001FA00-\\U0001FA6F\"  \n        \"\\U0001FA70-\\U0001FAFF\"  \n        \"\\U00002702-\\U000027B0\"  \n        \"\\U000024C2-\\U0001F251\"  \n        \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\nenglish_words = set(words.words())\n\ndef remove_non_english_words(text):\n    tokens = word_tokenize(text)\n    return ' '.join([word for word in tokens if word.lower() in english_words])\n\n\n\n\ndef remove_extra_spaces(sentence):\n    sentence = sentence.strip()\n    sentence = re.sub(r'\\s+', ' ', sentence)\n    return sentence\n\n\ndef remove_digits(st):\n    st = ''.join([i for i in st if not i.isdigit()])\n    return st\n\ndef digit_clean(st):\n    st = remove_digits(st)\n    return st\n\ndef remove_punctuation(text):\n    return re.sub(r'[^\\w\\s]', '', text)\n\n\ndef data_cleaning(text):\n    text = str(text).lower()\n    text = remove_urls(text)\n    text = remove_usernames(text)\n    text = remove_hashtags(text)\n    text = remove_emojis(text)\n    text = digit_clean(text)\n    text = remove_punctuation(text)\n    text = remove_non_english_words(text)\n    text = remove_extra_spaces(text)\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_datacleaning = []\nfor i in range(len(x_shen)):\n    x_datacleaning.append(data_cleaning(x_shen[i]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# random_state=42\nfrom sklearn.model_selection import train_test_split\nX_train_validation, X_test, y_train_validation, y_test = train_test_split( x_datacleaning, y_shen, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\npattern = r\"\\b(im|i\\swas|i\\sam|ive\\sbeen|i\\shave\\sbeen)\\b.*?\\b(diagnoses|diagnose|diagnosed)\\b.*?\\bdepression\\b\"\n\nx_removekeyword = [re.sub(pattern, \"\", text, flags=re.IGNORECASE).strip() for text in X_train_validation]\ntest_remove_keyword = [re.sub(pattern, \"\", text, flags=re.IGNORECASE).strip() for text in X_test]\n\nx_removekeyword = [sentence.replace('depression', '').replace('diagnosed', '') for sentence in x_removekeyword]\ntest_remove_keyword = [sentence.replace('depression', '').replace('diagnosed', '') for sentence in test_remove_keyword]\n\n\npatterns_to_remove = [r'\\bim\\b', r'\\bi was\\b', r'\\bi am\\b', r'\\bive been\\b', r'\\bi have been\\b']\n\nx_removekeyword = [re.sub('|'.join(patterns_to_remove), '', sentence, flags=re.IGNORECASE).strip() \n                   for sentence in x_removekeyword]\ntest_remove_keyword = [re.sub('|'.join(patterns_to_remove), '', sentence, flags=re.IGNORECASE).strip() \n                   for sentence in test_remove_keyword]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel\n\n\n\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertModel.from_pretrained(model_name)\n\n\ntexts =x_removekeyword\n\ntokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n\n\nwith torch.no_grad():\n        outputs = model(**tokenized_texts)\n\n\nsequence_output = outputs.last_hidden_state\n\ndel tokenized_texts\ndel outputs\n\n\nprint(\"Shape of sequence output:\", sequence_output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_shape = (sequence_output.shape[1], sequence_output.shape[2])\nsequence_output_np = sequence_output.numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras import layers, models\nfrom scikeras.wrappers import KerasClassifier\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, Dropout, Dense, Flatten, LSTM,InputLayer, Attention,Input, Reshape\nfrom sklearn.model_selection import GridSearchCV, KFold, train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom keras.regularizers import l2, l1\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential, Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom keras.activations import tanh\nfrom keras.optimizers import *\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dropout, Dense, Attention, Reshape\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Reshape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, x_validation, y_train, y_validation = train_test_split(sequence_output_np, y_train_validation, test_size=0.125)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#bilstm\n\nfrom tensorflow.keras.layers import Lambda\nmodel = Sequential()\n\n\nmodel.add(Bidirectional(LSTM(units=32, activation='relu', return_sequences=True, kernel_regularizer=l2(0.01), input_shape=input_shape)))\nmodel.add(Dropout(0.4))\nmodel.add(Lambda(lambda x: Attention()([x, x])))\nmodel.add(Flatten())\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()\n\nmodel.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_data=(x_validation, y_validation))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport numpy as np\n\narray1 = X_test\narray2 = test_remove_keyword\n\nhalf_len1 = len(array1) // 2\nhalf_len2 = len(array2) // 2\n\nfirst_half_array1 = array1[:half_len1]\n\nsecond_half_array2 = array2[half_len2:]\n\narraytotal = np.concatenate((first_half_array1, second_half_array2))\n\n\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel_bert = BertModel.from_pretrained(model_name)\n\nmax_sequence_length = sequence_output.shape[1]\n\ntexts = arraytotal\n\ntokenized_texts = [tokenizer.encode(text, add_special_tokens=True)[:max_sequence_length] for text in texts]\n\npadded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in tokenized_texts], batch_first=True, padding_value=tokenizer.pad_token_id)\n\n\nattention_masks = torch.where(padded_texts != tokenizer.pad_token_id, torch.ones_like(padded_texts), torch.zeros_like(padded_texts))\n\ninputs = {'input_ids': padded_texts, 'attention_mask': attention_masks}\n\nwith torch.no_grad():\n    outputs = model_bert(**inputs)\n\nx_test = outputs.last_hidden_state\n\n\npadded_x_test = torch.nn.functional.pad(x_test, (0, 0, 0, max_sequence_length - x_test.shape[1], 0, 0), value=0)\n\nprint(\"Shape of sequence output:\", padded_x_test.shape)\n\n\nx_test_np = padded_x_test.numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_test_pred = model.predict(x_test_np)\ny_test_pred_binary = (y_test_pred > 0.5).astype(int)\n\ntest_accuracy = accuracy_score(y_test, y_test_pred_binary)\nprint(f'Accuracy: {test_accuracy:.4f}')\n\ntest_f1_score = f1_score(y_test, y_test_pred_binary)\nprint(f'F1 Score: {test_f1_score:.4f}')\n\ntest_precision = precision_score(y_test, y_test_pred_binary)\nprint(f'Precision: {test_precision:.4f}')\n\ntest_recall = recall_score(y_test, y_test_pred_binary)\nprint(f'Recall: {test_recall:.4f}')\n\n\nmatrix = confusion_matrix(y_test, y_test_pred_binary)\n\n\nTN = matrix[0, 0]\nFP = matrix[0, 1]\nFN = matrix[1, 0]\nTP = matrix[1, 1]\n\n\nspecificity = TN / (TN + FP)\nsensitivity = TP / (TP + FN)\n\n\nerror_rate = (FP + FN) / (TP + TN + FP + FN)\n\nprint(\"Specificity:\", f'{specificity:.4f}')\nprint(\"Sensitivity:\", f'{sensitivity:.4f}')\nprint(\"Error Rate:\", f'{error_rate:.4f}')\n\n\nclass_accuracy = matrix.diagonal() / matrix.sum(axis=1)\n\n\nfor i, acc in enumerate(class_accuracy):\n    print(f'Accuracy for class {i}: {acc:.4f}')\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}